{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZayjygAbLpE"
      },
      "outputs": [],
      "source": [
        "!pip install snowflake snowflake-ml-python snowflake-snowpark-python python-dotenv langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyOc8CI1biRi"
      },
      "outputs": [],
      "source": [
        "!pip install crewai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDmCUW4i2Rmy",
        "outputId": "904ae01f-8999-440f-ff58-ed2110c95488"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "snowflake                                1.0.2\n",
            "snowflake-connector-python               3.12.4\n",
            "snowflake.core                           1.0.2\n",
            "snowflake._legacy                        1.0.0\n",
            "snowflake-ml-python                      1.7.2\n",
            "snowflake-snowpark-python                1.26.0\n",
            "langchain                                0.3.13\n",
            "langchain-cohere                         0.3.4\n",
            "langchain-community                      0.3.13\n",
            "langchain-core                           0.3.28\n",
            "langchain-experimental                   0.3.4\n",
            "langchain-openai                         0.2.14\n",
            "langchain-text-splitters                 0.3.3\n",
            "python-dotenv                            1.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip list | grep -i \"snowflake\"\n",
        "!pip list | grep -i \"langchain\"\n",
        "!pip list | grep -i \"python-dotenv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VPzKw9a2rTD",
        "outputId": "36d8d67b-0957-4576-d6af-068dc2eab9c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "llama-index                              0.12.8\n",
            "llama-index-agent-openai                 0.4.1\n",
            "llama-index-cli                          0.4.0\n",
            "llama-index-core                         0.12.8\n",
            "llama-index-embeddings-huggingface       0.4.0\n",
            "llama-index-embeddings-openai            0.3.1\n",
            "llama-index-indices-managed-llama-cloud  0.6.3\n",
            "llama-index-llms-openai                  0.3.12\n",
            "llama-index-multi-modal-llms-openai      0.4.1\n",
            "llama-index-program-openai               0.3.1\n",
            "llama-index-question-gen-openai          0.3.0\n",
            "llama-index-readers-file                 0.4.1\n",
            "llama-index-readers-github               0.5.0\n",
            "llama-index-readers-llama-parse          0.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip list | grep -i \"llama-index\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5RIFyrz3fLE",
        "outputId": "ee351387-8416-46ee-b91b-016d550f9029"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain-mistralai\n",
            "  Downloading langchain_mistralai-0.2.4-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting llama-index-readers-github\n",
            "  Downloading llama_index_readers_github-0.5.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting llama-index\n",
            "  Downloading llama_index-0.12.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting llama-index-embeddings-huggingface\n",
            "  Downloading llama_index_embeddings_huggingface-0.4.0-py3-none-any.whl.metadata (767 bytes)\n",
            "Requirement already satisfied: httpx<1,>=0.25.2 in /usr/local/lib/python3.10/dist-packages (from langchain-mistralai) (0.27.2)\n",
            "Requirement already satisfied: httpx-sse<1,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from langchain-mistralai) (0.4.0)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.27 in /usr/local/lib/python3.10/dist-packages (from langchain-mistralai) (0.3.28)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-mistralai) (2.10.3)\n",
            "Requirement already satisfied: tokenizers<1,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from langchain-mistralai) (0.20.3)\n",
            "Collecting llama-index-core<0.13.0,>=0.12.0 (from llama-index-readers-github)\n",
            "  Downloading llama_index_core-0.12.8-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting llama-index-readers-file<0.5.0,>=0.4.0 (from llama-index-readers-github)\n",
            "  Downloading llama_index_readers_file-0.4.1-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting llama-index-agent-openai<0.5.0,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_agent_openai-0.4.1-py3-none-any.whl.metadata (726 bytes)\n",
            "Collecting llama-index-cli<0.5.0,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_cli-0.4.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting llama-index-embeddings-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl.metadata (684 bytes)\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.6.3-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting llama-index-llms-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_llms_openai-0.3.12-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.5.0,>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.4.1-py3-none-any.whl.metadata (726 bytes)\n",
            "Collecting llama-index-program-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_program_openai-0.3.1-py3-none-any.whl.metadata (764 bytes)\n",
            "Collecting llama-index-question-gen-openai<0.4.0,>=0.3.0 (from llama-index)\n",
            "  Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl.metadata (783 bytes)\n",
            "Collecting llama-index-readers-llama-parse>=0.4.0 (from llama-index)\n",
            "  Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.27.0)\n",
            "Requirement already satisfied: sentence-transformers>=2.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-embeddings-huggingface) (3.3.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.25.2->langchain-mistralai) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.25.2->langchain-mistralai) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.25.2->langchain-mistralai) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.25.2->langchain-mistralai) (3.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.25.2->langchain-mistralai) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.25.2->langchain-mistralai) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2023.12.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.12.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.11.10)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-mistralai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-mistralai) (0.1.147)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain-mistralai) (9.0.0)\n",
            "Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.58.1)\n",
            "Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-github) (2.0.36)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-github) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-github) (1.2.15)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-github)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-github)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-github) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-github) (3.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-github) (1.26.4)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-github) (11.0.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-github) (0.7.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-github) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.13.0,>=0.12.0->llama-index-readers-github) (1.17.0)\n",
            "Collecting llama-cloud>=0.1.5 (from llama-index-indices-managed-llama-cloud>=0.4.0->llama-index)\n",
            "  Downloading llama_cloud-0.1.7-py3-none-any.whl.metadata (860 bytes)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index-readers-github) (4.12.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index-readers-github) (2.2.2)\n",
            "Requirement already satisfied: pypdf<6.0.0,>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index-readers-github) (5.1.0)\n",
            "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.5.0,>=0.4.0->llama-index-readers-github)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting llama-parse>=0.5.0 (from llama-index-readers-llama-parse>=0.4.0->llama-index)\n",
            "  Downloading llama_parse-0.5.19-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>3.8.1->llama-index) (2024.11.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2->langchain-mistralai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2->langchain-mistralai) (2.27.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (4.47.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.13.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.18.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.5.0,>=0.4.0->llama-index-readers-github) (2.6)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.27->langchain-mistralai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-mistralai) (3.10.12)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-mistralai) (1.0.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.5.0,>=0.4.0->llama-index) (0.8.2)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.25.2->langchain-mistralai) (1.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.2.3)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-github) (3.1.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (1.3.0)\n",
            "Collecting tokenizers<1,>=0.15.1 (from langchain-mistralai)\n",
            "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (0.4.5)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-github) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.0->llama-index-readers-github) (3.23.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index-readers-github) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index-readers-github) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index-readers-github) (2024.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file<0.5.0,>=0.4.0->llama-index-readers-github) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.1->llama-index-embeddings-huggingface) (3.0.2)\n",
            "Downloading langchain_mistralai-0.2.4-py3-none-any.whl (15 kB)\n",
            "Downloading llama_index_readers_github-0.5.0-py3-none-any.whl (21 kB)\n",
            "Downloading llama_index-0.12.8-py3-none-any.whl (6.8 kB)\n",
            "Downloading llama_index_embeddings_huggingface-0.4.0-py3-none-any.whl (8.6 kB)\n",
            "Downloading llama_index_agent_openai-0.4.1-py3-none-any.whl (13 kB)\n",
            "Downloading llama_index_cli-0.4.0-py3-none-any.whl (27 kB)\n",
            "Downloading llama_index_core-0.12.8-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_index_embeddings_openai-0.3.1-py3-none-any.whl (6.2 kB)\n",
            "Downloading llama_index_indices_managed_llama_cloud-0.6.3-py3-none-any.whl (11 kB)\n",
            "Downloading llama_index_llms_openai-0.3.12-py3-none-any.whl (14 kB)\n",
            "Downloading llama_index_multi_modal_llms_openai-0.4.1-py3-none-any.whl (5.8 kB)\n",
            "Downloading llama_index_program_openai-0.3.1-py3-none-any.whl (5.3 kB)\n",
            "Downloading llama_index_question_gen_openai-0.3.0-py3-none-any.whl (2.9 kB)\n",
            "Downloading llama_index_readers_file-0.4.1-py3-none-any.whl (38 kB)\n",
            "Downloading llama_index_readers_llama_parse-0.4.0-py3-none-any.whl (2.5 kB)\n",
            "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading llama_cloud-0.1.7-py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.1/242.1 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llama_parse-0.5.19-py3-none-any.whl (15 kB)\n",
            "Downloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: striprtf, filetype, dirtyjson, tokenizers, llama-cloud, llama-index-core, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, langchain-mistralai, llama-index-readers-llama-parse, llama-index-readers-github, llama-index-multi-modal-llms-openai, llama-index-embeddings-huggingface, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.20.3\n",
            "    Uninstalling tokenizers-0.20.3:\n",
            "      Successfully uninstalled tokenizers-0.20.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "chromadb 0.5.23 requires tokenizers<=0.20.3,>=0.13.2, but you have tokenizers 0.21.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dirtyjson-1.0.8 filetype-1.2.0 langchain-mistralai-0.2.4 llama-cloud-0.1.7 llama-index-0.12.8 llama-index-agent-openai-0.4.1 llama-index-cli-0.4.0 llama-index-core-0.12.8 llama-index-embeddings-huggingface-0.4.0 llama-index-embeddings-openai-0.3.1 llama-index-indices-managed-llama-cloud-0.6.3 llama-index-llms-openai-0.3.12 llama-index-multi-modal-llms-openai-0.4.1 llama-index-program-openai-0.3.1 llama-index-question-gen-openai-0.3.0 llama-index-readers-file-0.4.1 llama-index-readers-github-0.5.0 llama-index-readers-llama-parse-0.4.0 llama-parse-0.5.19 striprtf-0.0.26 tokenizers-0.21.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain-mistralai llama-index-readers-github llama-index llama-index-embeddings-huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqz-Ok4J5JKq"
      },
      "outputs": [],
      "source": [
        "!python -v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQTs0MFL5PhK",
        "outputId": "9f5fe2bb-2d5e-4b22-b14f-84381a7aa088"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHSz3A8vANu7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doCncXp-3Vyj",
        "outputId": "729f1b34-02b3-4ee2-c2f9-f3fd720fd755"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.41.1-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (11.0.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.29.2)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (17.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.9.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (1.18.4)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.12.14)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.22.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.41.1-py2.py3-none-any.whl (9.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.41.1 watchdog-6.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYW0_f3pbF8K",
        "outputId": "0b66fb8b-1d3c-44ed-f5ac-d934bf0b7ee8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_config.py:345: UserWarning: Valid config keys have changed in V2:\n",
            "* 'fields' has been removed\n",
            "  warnings.warn(message, UserWarning)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from typing import List\n",
        "from crewai import Agent, Task, Crew\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from snowflake.snowpark.session import Session\n",
        "from snowflake.core import Root\n",
        "import tempfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "-M75T_J6bnJu"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata  # for Google Colab authentication\n",
        "\n",
        "connection_params = {\n",
        "    \"account\": userdata.get('SNOWFLAKE_ACCOUNT'),\n",
        "    \"user\": userdata.get('SNOWFLAKE_USER'),\n",
        "    \"password\": userdata.get('SNOWFLAKE_USER_PASSWORD'),\n",
        "    \"role\": userdata.get('SNOWFLAKE_ROLE'),\n",
        "    \"database\": userdata.get('SNOWFLAKE_DATABASE'),\n",
        "    \"schema\": userdata.get('SNOWFLAKE_SCHEMA'),\n",
        "    \"warehouse\": userdata.get('SNOWFLAKE_WAREHOUSE')\n",
        "}\n",
        "snowpark_session = Session.builder.configs(connection_params).create()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwCN-XJYAUQ4",
        "outputId": "5b6c92a5-c4da-4ec8-cafb-109390acc492"
      },
      "outputs": [],
      "source": [
        "connection_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8LHql0hka-7e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import List\n",
        "from crewai import Agent, Task, Crew\n",
        "from langchain.tools import Tool\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from snowflake.snowpark.session import Session\n",
        "from snowflake.core import Root\n",
        "import tempfile\n",
        "\n",
        "class PDFProcessingAgent:\n",
        "    def __init__(self, snowpark_session: Session):\n",
        "        self.session = snowpark_session\n",
        "        self.root = Root(self.session)\n",
        "\n",
        "        # Initialize Snowflake table for storing PDF chunks\n",
        "        self.init_snowflake_storage()\n",
        "\n",
        "    def init_snowflake_storage(self):\n",
        "        \"\"\"Initialize Snowflake table for storing PDF text chunks\"\"\"\n",
        "        # Set the database context\n",
        "        self.session.sql(\"USE DATABASE BADSCIENTIST_DEMO\").collect()\n",
        "        self.session.sql(\"USE SCHEMA NOTEBOOKS\").collect()\n",
        "\n",
        "        # Create table for storing PDF chunks\n",
        "        self.session.sql(\"\"\"\n",
        "            CREATE TABLE IF NOT EXISTS pdf_docs (\n",
        "                id INTEGER AUTOINCREMENT,\n",
        "                doc_text STRING,\n",
        "                metadata VARIANT\n",
        "            )\n",
        "        \"\"\").collect()\n",
        "\n",
        "        # Create or replace Cortex Search service\n",
        "        self.session.sql(\"\"\"\n",
        "            CREATE OR REPLACE CORTEX SEARCH SERVICE BADSCIENTIST_DEMO.NOTEBOOKS.pdf_docs_svc\n",
        "            ON doc_text\n",
        "            WAREHOUSE = compute_wh\n",
        "            TARGET_LAG = '1 hour'\n",
        "            AS\n",
        "                SELECT\n",
        "                    doc_text\n",
        "                FROM pdf_docs\n",
        "        \"\"\").collect()\n",
        "\n",
        "    def process_pdf(self, pdf_path: str) -> List[str]:\n",
        "        \"\"\"Process PDF file and split into chunks\"\"\"\n",
        "        loader = PyPDFLoader(pdf_path)\n",
        "        pages = loader.load()\n",
        "\n",
        "        # Split pages into smaller chunks for better retrieval\n",
        "        chunks = []\n",
        "        for page in pages:\n",
        "            # Split into paragraphs or smaller segments\n",
        "            text_chunks = self._split_into_chunks(page.page_content)\n",
        "            chunks.extend(text_chunks)\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def _split_into_chunks(self, text: str, chunk_size: int = 1000) -> List[str]:\n",
        "        \"\"\"Split text into smaller chunks\"\"\"\n",
        "        words = text.split()\n",
        "        chunks = []\n",
        "        current_chunk = []\n",
        "        current_size = 0\n",
        "\n",
        "        for word in words:\n",
        "            if current_size + len(word) > chunk_size:\n",
        "                chunks.append(\" \".join(current_chunk))\n",
        "                current_chunk = [word]\n",
        "                current_size = len(word)\n",
        "            else:\n",
        "                current_chunk.append(word)\n",
        "                current_size += len(word)\n",
        "\n",
        "        if current_chunk:\n",
        "            chunks.append(\" \".join(current_chunk))\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def store_chunks(self, chunks: List[str]):\n",
        "        \"\"\"Store text chunks in Snowflake\"\"\"\n",
        "        for chunk in chunks:\n",
        "            # Insert chunk into Snowflake\n",
        "            self.session.sql(\"\"\"\n",
        "                INSERT INTO pdf_docs (doc_text, metadata)\n",
        "                SELECT column1, PARSE_JSON(column2)\n",
        "                FROM VALUES(?, ?)\n",
        "            \"\"\", params=(chunk, '{}')).collect()\n",
        "\n",
        "class PDFQueryAgent:\n",
        "    def __init__(self, snowpark_session: Session):\n",
        "        self.session = snowpark_session\n",
        "        self.root = Root(self.session)\n",
        "\n",
        "    def query(self, question: str, num_chunks: int = 4) -> dict:\n",
        "        \"\"\"Query the stored PDF content using Cortex Search\"\"\"\n",
        "        # Get relevant chunks using Cortex Search\n",
        "        search_service = (\n",
        "            self.root\n",
        "            .databases[self.session.get_current_database()]\n",
        "            .schemas[self.session.get_current_schema()]\n",
        "            .cortex_search_services['pdf_docs_svc']\n",
        "        )\n",
        "\n",
        "        results = search_service.search(\n",
        "            query=question,\n",
        "            columns=[\"doc_text\"],\n",
        "            limit=num_chunks\n",
        "        )\n",
        "\n",
        "        context = \"\\n\\n\".join([r[\"doc_text\"] for r in results.results])\n",
        "\n",
        "        # Generate response using Snowflake's LLM\n",
        "        prompt = f\"\"\"\n",
        "        Based on the following context, answer the question.\n",
        "\n",
        "        Context:\n",
        "        {context}\n",
        "\n",
        "        Question: {question}\n",
        "\n",
        "        Answer:\n",
        "        \"\"\"\n",
        "        print(prompt)\n",
        "        response = self.session.sql(\n",
        "            \"SELECT snowflake.cortex.complete(?, ?)\",\n",
        "            params=(\"mistral-large\", prompt)\n",
        "        ).collect()[0][0]\n",
        "\n",
        "        return {\n",
        "            'question': question,\n",
        "            'context': context,\n",
        "            'response': response\n",
        "        }\n",
        "\n",
        "def create_pdf_tools(snowpark_session: Session):\n",
        "    \"\"\"Create LangChain tools for PDF processing and querying\"\"\"\n",
        "    pdf_processor = PDFProcessingAgent(snowpark_session)\n",
        "    pdf_querier = PDFQueryAgent(snowpark_session)\n",
        "\n",
        "    process_tool = Tool(\n",
        "        name=\"process_pdf\",\n",
        "        func=lambda pdf_path: pdf_processor.process_pdf(pdf_path),\n",
        "        description=\"Process a PDF file and split it into chunks\"\n",
        "    )\n",
        "\n",
        "    store_tool = Tool(\n",
        "        name=\"store_chunks\",\n",
        "        func=lambda chunks: pdf_processor.store_chunks(chunks),\n",
        "        description=\"Store text chunks in Snowflake database\"\n",
        "    )\n",
        "\n",
        "    query_tool = Tool(\n",
        "        name=\"query_pdf\",\n",
        "        func=lambda question: pdf_querier.query(question),\n",
        "        description=\"Query PDF content and get answers\"\n",
        "    )\n",
        "\n",
        "    return [process_tool, store_tool, query_tool]\n",
        "\n",
        "# Create CREW AI agents\n",
        "def create_pdf_crew(snowpark_session: Session):\n",
        "    tools = create_pdf_tools(snowpark_session)\n",
        "\n",
        "    # PDF Processing Agent\n",
        "    pdf_processor = Agent(\n",
        "        name=\"PDF Processor\",\n",
        "        role=\"Processes and stores PDF documents in Snowflake\",\n",
        "        goal=\"Efficiently process PDFs and store them for later retrieval\",\n",
        "        backstory=\"Expert at handling PDFs and managing document storage\",\n",
        "        tools=tools[:2]  # process_pdf and store_chunks tools\n",
        "    )\n",
        "\n",
        "    # Query Agent\n",
        "    query_agent = Agent(\n",
        "        name=\"Query Expert\",\n",
        "        role=\"Answers questions about stored PDF content\",\n",
        "        goal=\"Provide accurate answers based on stored PDF content\",\n",
        "        backstory=\"Specialist in information retrieval and question answering\",\n",
        "        tools=[tools[2]]  # query_pdf tool\n",
        "    )\n",
        "\n",
        "    # Create crew\n",
        "    crew = Crew(\n",
        "        agents=[pdf_processor, query_agent],\n",
        "        tasks=[\n",
        "            Task(\n",
        "                description=\"Process and store PDF documents\",\n",
        "                agent=pdf_processor,\n",
        "                expected_output=\"A confirmation message that the PDF has been processed and stored in the database\"\n",
        "            ),\n",
        "            Task(\n",
        "                description=\"Answer questions about PDF content\",\n",
        "                agent=query_agent,\n",
        "                expected_output=\"A detailed answer to the user's question based on the content stored in the database\"\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return crew"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "soKhV436iLmv"
      },
      "outputs": [],
      "source": [
        "def handle_pdf_upload(pdf_file):\n",
        "    try:\n",
        "        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:\n",
        "            tmp_file.write(pdf_file.read())\n",
        "            tmp_path = tmp_file.name\n",
        "\n",
        "        processor = PDFProcessingAgent(snowpark_session)\n",
        "        chunks = processor.process_pdf(tmp_path)\n",
        "        processor.store_chunks(chunks)\n",
        "        os.unlink(tmp_path)\n",
        "\n",
        "        print(f\"Successfully processed PDF and stored {len(chunks)} chunks\")\n",
        "        return \"PDF processed and stored successfully!\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing PDF: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Query the content\n",
        "def ask_question(question: str):\n",
        "    query_agent = PDFQueryAgent(snowpark_session)\n",
        "    result = query_agent.query(question)\n",
        "    return result['response']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4A9i_I5ajWE0",
        "outputId": "41684c3d-96e5-483c-b4c1-996b3fc00f54"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:opentelemetry.trace:Overriding of current TracerProvider is not allowed\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully processed PDF and stored 68 chunks\n",
            "PDF processed and stored successfully!\n"
          ]
        }
      ],
      "source": [
        "# Initialize\n",
        "snowpark_session = Session.builder.configs(connection_params).create()\n",
        "crew = create_pdf_crew(snowpark_session)\n",
        "\n",
        "# Process a PDF\n",
        "with open('/content/RAG-paper.pdf', 'rb') as f:\n",
        "    result = handle_pdf_upload(f)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjhTHnzvj93i",
        "outputId": "90f400c2-cad6-4ba8-c91c-5ccd52e93ca4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "        Based on the following context, answer the question.\n",
            "        \n",
            "        Context:\n",
            "        performance as more documents are retrieved. Center: Retrieval recall perfor- mance in NQ. Right: MS-MARCO Bleu-1 and Rouge-L as more documents are retrieved. 5 Related Work Single-Task Retrieval Prior work has shown that retrieval improves performance across a variety of NLP tasks when considered in isolation. Such tasks include open-domain question answering [5, 29], fact checking [ 56], fact completion [ 48], long-form question answering [ 12], Wikipedia article generation [36], dialogue [ 41, 65, 9, 13], translation [ 17], and language modeling [ 19, 27]. Our work uniﬁes previous successes in incorporating retrieval into individual tasks, showing that a single retrieval-based architecture is capable of achieving strong performance across several tasks. 8\n",
            "\n",
            "z∈top-k(p(·|x)) pη(z|x)pθ(y|x,z) = ∑ z∈top-k(p(·|x)) pη(z|x) N∏ i pθ(yi|x,z,y 1:i−1) RAG-Token Model In the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer. Concretely, the top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing, and repeating the process with the following output token, Formally, we deﬁne: pRAG-Token(y|x) ≈ N∏ i ∑ z∈top-k(p(·|x)) pη(z|x)pθ(yi|x,z,y 1:i−1) Finally, we note that RAG can be used for sequence classiﬁcation tasks by considering the target class as a target sequence of length one, in which case RAG-Sequence and RAG-Token are equivalent. 2.2 Retriever: DPR The retrieval component pη(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture: pη(z|x) ∝exp ( d(z)⊤q(x) ) d(z) =BERTd(z), q(x) =BERTq(x) where d(z) is a dense representation of a document produced by a BERTBASE document encoder [8], and q(x) a query representation produced by a query encoder, also based on\n",
            "\n",
            "Navigable Small World approximation for fast retrieval [37]. During training, we retrieve the top kdocuments for each query. We consider k∈{5,10}for training and set kfor test time using dev data. We now discuss experimental details for each task. 3.1 Open-domain Question Answering Open-domain question answering (QA) is an important real-world application and common testbed for knowledge-intensive tasks [20]. We treat questions and answers as input-output text pairs (x,y) and train RAG by directly minimizing the negative log-likelihood of answers. We compare RAG to the popular extractive QA paradigm [5, 7, 31, 26], where answers are extracted spans from retrieved documents, relying primarily on non-parametric knowledge. We also compare to “Closed-Book QA” approaches [52], which, like RAG, generate answers, but which do not exploit retrieval, instead relying purely on parametric knowledge. We consider four popular open-domain QA datasets: Natural Questions (NQ) [29], TriviaQA (TQA) [24]. WebQuestions (WQ) [3] and CuratedTrec (CT) [2]. As CT and WQ are small, we follow DPR [26] by initializing CT and WQ models with our NQ RAG model. We use the same\n",
            "\n",
            "generation, and we ﬁnd that our models generate responses that are more factual, speciﬁc, and diverse than a BART baseline. For FEVER [56] fact veriﬁcation, we achieve results within 4.3% of state-of-the-art pipeline models which use strong retrieval supervision. Finally, we demonstrate that the non-parametric memory can be replaced to update the models’ knowledge as the world changes.1 2 Methods We explore RAG models, which use the input sequencex to retrieve text documents z and use them as additional context when generating the target sequence y . As shown in Figure 1, our models leverage two components: (i) a retriever p η(z |x ) with parameters η that returns (top-K truncated) distributions over text passages given a query x and (ii) a generator p θ(y i |x,z,y 1:i −1) parametrized 1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform- ers Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/ examples/rag/. An interactive demo of RAG models can be found at https://huggingface.co/rag/ 2\n",
            "        \n",
            "        Question: What is retrieval, how can I use python to code this concept\n",
            "        \n",
            "        Answer:\n",
            "        \n",
            " Retrieval in the context of Natural Language Processing (NLP) refers to the process of obtaining relevant documents or passages from a large corpus based on a given query or input. It is a crucial component in tasks such as open-domain question answering, fact checking, and dialogue systems.\n",
            "\n",
            "To implement retrieval in Python, you would typically need to follow these steps:\n",
            "\n",
            "1. **Text Preprocessing**: This involves cleaning and transforming raw text data into a format that can be understood by the machine. Python libraries like NLTK and spaCy can be used for this purpose.\n",
            "\n",
            "2. **Document Embedding**: This is the process of converting text into numerical vectors that the machine can understand. You can use pre-trained models like BERT (Bidirectional Encoder Representations from Transformers) for this purpose. In the provided context, the document representation `d(z)` and query representation `q(x)` are obtained using BERT.\n",
            "\n",
            "3. **Similarity Score Calculation**: Once you have the document and query embeddings, you can calculate similarity scores between them. The provided context uses a dot product for this purpose.\n",
            "\n",
            "4. **Retrieval of Top Documents**: Based on the similarity scores, you can retrieve the top k documents. The number of documents retrieved (k) can be set based on your requirements or determined using validation data.\n",
            "\n",
            "Here's a simplified example of how you might implement retrieval in Python:\n",
            "\n",
            "```python\n",
            "from sentence_transformers import SentenceTransformer\n",
            "import torch\n",
            "\n",
            "# Load pre-trained model\n",
            "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
            "\n",
            "# Preprocess text (assuming this step has been done)\n",
            "documents = [\"Document 1\", \"Document 2\", \"Document 3\"]\n",
            "queries = [\"Query 1\", \"Query 2\"]\n",
            "\n",
            "# Get document and query embeddings\n",
            "document_embeddings = model.encode(documents)\n",
            "query_embeddings = model.encode(queries)\n",
            "\n",
            "# Calculate similarity scores\n",
            "scores = torch.mm(query_embeddings, document_embeddings.t())\n",
            "\n",
            "# Retrieve top k documents\n",
            "k = 2\n",
            "top_documents = [sorted(list(enumerate(scores[i])), key=lambda x:x[1], reverse=True)[:k] for i in range(len(queries))]\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "# Ask questions\n",
        "response = ask_question(\"What is retrieval, how can I use python to code this concept\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "492EnkLEknL-"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
